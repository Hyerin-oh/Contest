{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dev-Matching.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOg3KMyg_6DB"
      },
      "source": [
        "# 설치 목록\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVphsP283xmQ"
      },
      "source": [
        "# ! pip install albumentations==0.4.6\n",
        "# ! pip install wandb\n",
        "# ! pip install timm\n",
        "# !pip install MADGRAD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpownU985IOe"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ti4_9bma9ONd"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# 시각화를 위한 라이브러리\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "import glob\n",
        "import os\n",
        "plt.rcParams['axes.grid'] = False\n",
        "\n",
        "import numpy as np\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryq8AdaRAjb_"
      },
      "source": [
        "# Le's Go~!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "YnGrz5gk-AB4",
        "outputId": "7a2fddb6-4e9e-4edf-9db9-93051b0ac417"
      },
      "source": [
        "classes = ['dog', 'elephant', 'giraffe','guitar','horse','house','person']\n",
        "num_classes = []\n",
        "for name in classes :\n",
        "  path = '/content/input/train/' + name\n",
        "  num_classes.append(len(glob.glob(os.path.join(path, '*.jpg'))))\n",
        "print(num_classes)\n",
        "plt.bar(range(len(num_classes)), num_classes)\n",
        "ax = plt.subplot()\n",
        "ax.set_xticks([0, 1, 2, 3, 4, 5, 6,])\n",
        "ax.set_xticklabels(classes, rotation=30)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[329, 205, 235, 134, 151, 245, 399]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAESCAYAAAAcxXWZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deUBU5f4G8GcGBdyQwKxBTY0KUUNL6mouuQaaWNYtkastuFaQiXjFBfC6RCAumHvZKmkuKaEGpmamWe4iV1NxV0aQTUHZZub7+8Mf54obqMxi5/n8JeedM/OdceY573nPe87RiIiAiIj+1rTWLoCIiMyPYU9EpAIMeyIiFWDYExGpAMOeiEgFGPZERCrAsCciUoFq1i7gTnJzr8Bk4mkARESVodVq8NBDtW7ZZtNhbzIJw56IqApwGIeISAUY9kREKsCwJyJSgbsK+zlz5sDDwwNHjx4FAOzfvx99+vSBj48PAgMDkZ2drTz2Tm1ERGRZlQ77//73v9i/fz8aNGgAADCZTBg9ejQiIiKQnJwMb29vxMbGVthGRESWV6mwLykpwaRJkzBx4kRlWWpqKhwcHODt7Q0A8Pf3R1JSUoVtRERkeZUK+7i4OPTp0wcNGzZUlun1eri5uSl/u7i4wGQyIS8v745tRERkeRXOs9+3bx9SU1MRGhpqiXqIiCyqjlMNODrYzilHRcUG5F8urPLnrfAd7tq1C8ePH0e3bt0AABcuXMCgQYMwcOBApKenK4/LycmBVquFs7MzdDrdbduIiGyJo0M1+I1KsHYZisTpryDfDM9b4TDO0KFDsW3bNmzevBmbN2/Go48+isWLF2Pw4MEoKirC7t27AQDLli2Dr68vAKBly5a3bSMiIsu7530XrVaLmJgYREZGori4GA0aNMC0adMqbCMiIsvT2PINx7OzC3htHCIyq4cfrmNzwzgXL97bQI5Wq4Gra+1bt91PUURE9GBg2BMRqQDDnohIBRj2REQqwLAnIlIBhj0RkQow7ImIVIBhT0SkAgx7IiIVYNgTEakAw56ISAUY9kREKsCwJyJSAYY9EZEKMOyJiFSAYU9EpAIMeyIiFajUbQnff/99nDt3DlqtFjVr1kR4eDg8PT3RtWtX2Nvbw8HBAQAQGhqKjh07AgD279+PiIiIcrcldHV1Nd87ISKi26pU2EdHR6NOnToAgI0bN2LcuHFYvXo1AGD27Nl46qmnyj3eZDJh9OjRiIqKgre3N+bNm4fY2FhERUVVcflERFQZlRrGKQt6ACgoKIBGo7nj41NTU+Hg4ABvb28AgL+/P5KSku6jTCIiuh+V6tkDwPjx47F9+3aICD7//HNleWhoKEQEbdq0QUhICJycnKDX6+Hm5qY8xsXFBSaTCXl5eXB2dq7ad0BERBWq9AHaqVOnYsuWLRg5ciRiYmIAAPHx8fjxxx+xatUqiAgmTZpktkKJiOje3fVsnFdffRV//vkncnNzodPpAAD29vYICAjA3r17AQA6nQ7p6enKOjk5OdBqtezVExFZSYVhf+XKFej1euXvzZs3o27dunBwcEB+fj4AQESwfv16eHp6AgBatmyJoqIi7N69GwCwbNky+Pr6mqN+IiKqhArH7AsLCzFixAgUFhZCq9Wibt26WLBgAbKzsxEcHAyj0QiTyQR3d3dERkYCALRaLWJiYhAZGVlu6iUREVmHRkTE2kXcTnZ2AUwmmy2PiP4GHn64DvxGJVi7DEXi9Fdw8WL+Pa2r1Wrg6lr71m33UxQRET0YGPZERCrAsCciUgGGPRGRCjDsiYhUoNKXS3iQ1HGqAUcH23lrRcUG5F8utHYZRKRitpOIVcjRoZrNTaW6t4lURERVg8M4REQqwLAnIlIBhj0RkQow7ImIVIBhT0SkAgx7IiIVYNgTEakAw56ISAUY9kREKsCwJyJSgUpdLuH999/HuXPnoNVqUbNmTYSHh8PT0xMnT55EWFgY8vLy4OzsjOjoaDRp0gQA7thGRESWVamefXR0NH788UesWbMGgYGBGDduHAAgMjISAQEBSE5ORkBAACIiIpR17tRGRESWVamwr1OnjvLvgoICaDQaZGdn49ChQ+jduzcAoHfv3jh06BBycnLu2EZERJZX6atejh8/Htu3b4eI4PPPP4der8cjjzwCOzs7AICdnR3q168PvV4PEbltm4uLi3neCRER3ValD9BOnToVW7ZswciRIxETE2POmoiIqIrd9WycV199FX/++SceffRRZGRkwGg0AgCMRiMyMzOh0+mg0+lu20ZERJZXYdhfuXIFer1e+Xvz5s2oW7cuXF1d4enpibVr1wIA1q5dC09PT7i4uNyxjYiILK/CMfvCwkKMGDEChYWF0Gq1qFu3LhYsWACNRoOJEyciLCwM8+bNg5OTE6Kjo5X17tRGRESWVWHY16tXD8uXL79lm7u7O1asWHHXbUREZFk8g5aISAUY9kREKsCwJyJSAYY9EZEKMOyJiFSAYU9EpAIMeyIiFWDYExGpAMOeiEgFGPZERCrAsCciUgGGPRGRCjDsiYhUgGFPRKQCDHsiIhVg2BMRqQDDnohIBSq8U1Vubi7+/e9/48yZM7C3t0fjxo0xadIkuLi4wMPDA0899RS02mvbjJiYGHh4eAC4dq/amJgYGI1GtGjRAlFRUahRo4Z53w0REd1ShT17jUaDwYMHIzk5GYmJiWjUqBFiY2OV9mXLliEhIQEJCQlK0F+5cgXh4eFYsGABfv75Z9SqVQuLFy8237sgIqI7qjDsnZ2d8Y9//EP5u3Xr1khPT7/jOlu3bkXLli3RpEkTAIC/vz9++umn+6uUiIjuWYXDONczmUxYunQpunbtqiwbOHAgjEYjOnXqhODgYNjb20Ov18PNzU15jJubG/R6fdVVTUREd+WuDtBOnjwZNWvWxIABAwAAW7ZswQ8//ID4+HikpaVh7ty5ZimSiIjuT6XDPjo6GqdPn8asWbOUA7I6nQ4AULt2bbzxxhvYu3evsvz6oZ709HTlsUREZHmVGsaZMWMGUlNTsWjRItjb2wMALl26BAcHBzg6OsJgMCA5ORmenp4AgI4dO2Ly5Mk4deoUmjRpgmXLlqFnz57mexdEZDPqONWAo8NdjRCbVVGxAfmXC61dhtVV+D9y7NgxLFy4EE2aNIG/vz8AoGHDhhg8eDAiIiKg0WhgMBjwzDPPYMSIEQCu9fQnTZqEYcOGwWQywdPTE+PHjzfvOyEim+DoUA1+oxKsXYYicforyLd2ETagwrB/8sknceTIkVu2JSYm3na97t27o3v37vdeGRERVRmeQUtEpAIMeyIiFbCdoyj0QOFBOKIHi+38WumBwoNwRA8WDuMQEakAw56ISAUY9kREKsCwJyJSAYY9EZEKMOyJiFSAYU9EpAIMeyIiFWDYExGpAMOeiEgFGPZERCrAsCciUgGGPRGRClQY9rm5uRgyZAh8fHzg5+eHoKAg5OTkAAD279+PPn36wMfHB4GBgcjOzlbWu1MbERFZVoWXONZoNBg8eDD+8Y9/AACio6MRGxuLKVOmYPTo0YiKioK3tzfmzZuH2NhYREVFwWQy3baNbs2Wrg/Pa8MT/f1UmC7Ozs5K0ANA69atsXTpUqSmpsLBwQHe3t4AAH9/f3Tr1g1RUVF3bKNbs6Xrw/Pa8ER/P3c1Zm8ymbB06VJ07doVer0ebm5uSpuLiwtMJhPy8vLu2EZERJZ3V2E/efJk1KxZEwMGDDBXPUREZAaVHiSOjo7G6dOnsWDBAmi1Wuh0OqSnpyvtOTk50Gq1cHZ2vmMbERFZXqV69jNmzEBqairmzp0Le3t7AEDLli1RVFSE3bt3AwCWLVsGX1/fCtuIiMjyKuzZHzt2DAsXLkSTJk3g7+8PAGjYsCHmzp2LmJgYREZGori4GA0aNMC0adMAAFqt9rZtRERkeRWG/ZNPPokjR47csu3ZZ59FYmLiXbcREZFl8QxaIiIVYNgTEakAw56ISAUY9kREKsCwJyJSAYY9EZEKMOyJiFSAYU9EpAIMeyIiFWDYExGpAMOeiEgFGPZERCrAsCciUgGGPRGRCjDsiYhUgGFPRKQCDHsiIhWo1A3Ho6OjkZycjPPnzyMxMRFPPfUUAKBr166wt7eHg4MDACA0NBQdO3YEAOzfvx8RERHlbkvo6upqprdBRER3Uqmefbdu3RAfH48GDRrc1DZ79mwkJCQgISFBCXqTyYTRo0cjIiICycnJ8Pb2RmxsbNVWTkRElVapsPf29oZOp6v0k6ampsLBwQHe3t4AAH9/fyQlJd1bhUREdN8qNYxzJ6GhoRARtGnTBiEhIXBycoJer4ebm5vyGBcXF5hMJuTl5cHZ2fl+X5JINeo41YCjw33/TKtMUbEB+ZcLrV0G3YP7+hbFx8dDp9OhpKQEU6dOxaRJkzhcQ1SFHB2qwW9UgrXLUCROfwX51i6C7sl9zcYpG9qxt7dHQEAA9u7dqyxPT09XHpeTkwOtVstePRGRldxz2F+9ehX5+de28SKC9evXw9PTEwDQsmVLFBUVYffu3QCAZcuWwdfXtwrKJSKie1GpYZwpU6Zgw4YNyMrKwrvvvgtnZ2csWLAAwcHBMBqNMJlMcHd3R2RkJABAq9UiJiYGkZGR5aZeEhGRdVQq7CdMmIAJEybctHzNmjW3XefZZ59FYmLivVdGRERVhmfQEhGpAMOeiEgFGPZERCpgO2drEJkZT1AiNbOdbz6RmfEEJVIzDuMQEakAw56ISAUY9kREKsCwJyJSAYY9EZEKMOyJiFSAYU9EpAIMeyIiFWDYExGpAMOeiEgFGPZERCrAsCciUoEKwz46Ohpdu3aFh4cHjh49qiw/efIk+vXrBx8fH/Tr1w+nTp2qVBsREVlehWHfrVs3xMfHo0GDBuWWR0ZGIiAgAMnJyQgICEBERESl2oiIyPIqDHtvb2/odLpyy7Kzs3Ho0CH07t0bANC7d28cOnQIOTk5d2wjIiLruKfr2ev1ejzyyCOws7MDANjZ2aF+/frQ6/UQkdu2ubi4VF3lRERUaTxAS0SkAvfUs9fpdMjIyIDRaISdnR2MRiMyMzOh0+kgIrdtIyIi67innr2rqys8PT2xdu1aAMDatWvh6ekJFxeXO7YREZF1VNiznzJlCjZs2ICsrCy8++67cHZ2xrp16zBx4kSEhYVh3rx5cHJyQnR0tLLOndqIiMjyKgz7CRMmYMKECTctd3d3x4oVK265zp3aiIjI8niAlohIBRj2REQqwLAnIlIBhj0RkQow7ImIVIBhT0SkAgx7IiIVYNgTEakAw56ISAUY9kREKsCwJyJSAYY9EZEKMOyJiFSAYU9EpAIMeyIiFWDYExGpAMOeiEgF7umG49fr2rUr7O3t4eDgAAAIDQ1Fx44dsX//fkRERKC4uBgNGjTAtGnT4Orqet8FExHR3bvvsAeA2bNn46mnnlL+NplMGD16NKKiouDt7Y158+YhNjYWUVFRVfFyRER0l8wyjJOamgoHBwd4e3sDAPz9/ZGUlGSOlyIiokqokp59aGgoRARt2rRBSEgI9Ho93NzclHYXFxeYTCbk5eXB2dm5Kl6SiIjuwn337OPj4/Hjjz9i1apVEBFMmjSpKuoiIqIqdN9hr9PpAAD29vYICAjA3r17odPpkJ6erjwmJycHWq2WvXoiIiu5r7C/evUq8vPzAQAigvXr18PT0xMtW7ZEUVERdu/eDQBYtmwZfH19779aIiK6J/c1Zp+dnY3g4GAYjUaYTCa4u7sjMjISWq0WMTExiIyMLDf1koiIrOO+wr5Ro0ZYs2bNLdueffZZJCYm3s/TExFRFeEZtEREKsCwJyJSAYY9EZEKMOyJiFSAYU9EpAIMeyIiFWDYExGpAMOeiEgFGPZERCrAsCciUgGGPRGRCjDsiYhUgGFPRKQCDHsiIhVg2BMRqQDDnohIBRj2REQqYNawP3nyJPr16wcfHx/069cPp06dMufLERHRbZg17CMjIxEQEIDk5GQEBAQgIiLCnC9HRES3cV/3oL2T7OxsHDp0CF9++SUAoHfv3pg8eTJycnLg4uJSqefQajX3/Pr1H6pxz+uaQ2Xeiy3V/KDVC7BmS/k71vyg1Xsv62lERO61oDtJTU3FmDFjsG7dOmVZr169MG3aNLRo0cIcL0lERLfBA7RERCpgtrDX6XTIyMiA0WgEABiNRmRmZkKn05nrJYmI6DbMFvaurq7w9PTE2rVrAQBr166Fp6dnpcfriYio6phtzB4Ajh8/jrCwMFy+fBlOTk6Ijo7G448/bq6XIyKi2zBr2BMRkW3gAVoiIhVg2BMRqQDDnohIBRj2REQqwLAnIlIBhv1dKDtB7O/IYDAgLy/P2mX8bZhMJmuXQFQOw74SykLezs4OV69etXI1Va+0tBR79+7F6tWrAQDr169Henq6lasq78YNrS3PGBYRaLXXflqJiYk4ePCglSuq2IPYkRERZaNqy98HW8GwrwQ7OzsAwN69exEYGIg1a9YAePB7b2U/kOrVq6NmzZpYvXo1fHx8sHv3btSoYVtXAbSzs0NpaSnWrFmDc+fOobCwEIBt/sg1Gg3S0tKwaNEiJCcnw9nZ2dol3VbZd7jsO/6gdGZMJhM0Gg20Wi3y8vKQk5Nj7ZLuWmZmJi5evGix17ObOHHiRIu92gNqx44dGD58OC5fvozz58/j1KlT6NWrF6pVM9sVos3OaDQqvU8AOHPmDBISEtCgQQPExcWhRo0aNz3GklasWIHt27ejTZs2AK71kMeMGQMRwZ49e7Bv3z506NABGs29Xwa7qtz4OV26dAmvvvoqjEYjZsyYgXr16lmxujsr+/zWrl2L8ePH48iRIygtLcVjjz2mbABsUVnds2bNwpw5c5CWloaMjAw0bdoU9vb2Vq7uzkQEc+fORXh4OPbv349Lly6hVatWygbMXNizv8GNu7MlJSVYunQpgoKCMGnSJIwbNw516tTBokWLADx4vfvre3JXrlzBV199hWPHjqF169aIj49H06ZNlfdmrR/77NmzkZSUhIEDBwK41gP6888/8e2332Lo0KHYt28fCgsLld69tZTtVZR9ThcvXkRRURHq1q2LQYMG4cKFC7h69apNfUeuH/oo8+mnn2LTpk1YuHAhHnnkEXz88cc4dOiQlSq8tVt9hlOmTIHRaMTKlSuh1WqxYsUKpKWlWaG6ytu4cSNGjhyJmjVr4qeffsK7776L6OhoZGVlmb1jxbD/fwaDAcC1H25JSYny5dJqtcjNzVV+0F5eXujSpQt+/vlnpKenQ6vV2uRQwu2UfaE2btyIYcOGYdOmTZg1axa+/vprNGrUCK1atcK+ffuQmZkJ4NqtJS0hIyMDn3zyCU6dOoX69eujevXq+Oqrr5CcnIzjx48DAKKiovDRRx9h8ODBmDhxIuzs7Kzy2Zd9N8p6YYmJiejduzfi4uIQEhICABg2bBhEBFu2bLHa3tGNjEajMvRRUlICEUFJSQkKCgowZswYxMfHY/PmzQgJCUGrVq2sXa7i+j2nrKwsFBYWoqioCM7Oznj22WcxduxYnDp1ChMnTkTr1q2V37ItqlWrFpKSklC/fn3UqVMHHTp0QLdu3RAZGQnAvMOStvEttKL169cDgDIks2TJEvTv3x+RkZGYP38+qlWrBnd3d2RlZaGgoACOjo6oXr06NBoNvvnmGwCwiaGE2xGRcl+ggoICfPbZZ4iLi8PUqVPx7bffwtfXF3v37kVKSgq6d+8OnU6H0aNH491330VSUpLZe6Znz57FkCFD4OXlhSZNmuD48ePYsWMHDh8+DB8fHzg6OuL333+HTqfDypUr8corr6CkpATff/89zp8/b9barpeXl4e8vDwleIxGI7766its2LAB8+fPR//+/bF582Z89dVXAIChQ4fiu+++w4ULFyxW452UdVgWLVqE4cOHIzMzE/b29tixYwf69u0LR0dHfPvtt+jTpw/Onj2LrKwsK1d8jZ2dHfLy8jB27FhMnToVOTk5KC4uxk8//YSZM2eibdu2+Oabb+Dl5YVjx47h9OnT1i5ZkZGRgdjYWCQkJECv16Ndu3bo0aMHNm7cqDxm6tSp2LRpE3bs2GHWLFH9mL2fnx/q1auHli1bYsmSJfjll18wduxYNG3aFJMnT0arVq3g5uaGTZs24a+//oKjoyPi4+PRqlUrZGZm4rnnnrO5g5llynpEGo0GV65cgb29Pezs7KDX67Fx40a0a9cOjRo1Qp06daDX67Fv3z74+vrC29sber0eL730Et544w2zb8zq1q2LVatWoX79+ti2bRtcXFzQvHlzpKeno0+fPtDpdNi1axeAa3tgx44dw9ixY1GtWjX4+PiY/dhJ2ee4detWLF68GN27d0dMTAxatmyJ+vXr44033sB3332HJUuW4OWXX8YXX3yBV155BW3atMGSJUtgMBjw3HPPmbXGWynbyJf9/2VnZyMwMBB2dnb48MMP0ahRI6X9r7/+wsyZM1G9enVs2bIFERERaNiwIZ544gmL132jtLQ0DBkyBG3btkVYWBjq1q0LR0dHXLx4EVlZWRgyZAhq1aqFRYsWIS4uDm3btoWbm5u1y0ZCQgL+85//4IknnkBKSgpWrVqlhP348ePx4osvon79+nBwcIDJZEJxcTG8vLzMVo8qr3ppMpmU3tmqVasQFxeHTZs24YMPPkD//v3RpUsXAMDy5cvx7bffIjExEXv37sX333+PnJwcjBo1CqdOncLOnTtt/ibqpaWlmDFjBk6ePAkfHx+0a9cOTk5OiIuLQ0lJibL7uGPHDixcuBCvv/46/Pz8yj3H9Z9XVSjb2yh7zjNnziAgIAAGgwELFy5UNqTjxo1D27ZtMXjwYJw9exa//vorDh48iEuXLmHw4MHw9vausppuZ8+ePfjuu+8wffp0AECzZs3QsGFD+Pr64qOPPkK1atWwbds2rF69GlOmTEGNGjXQoUMHeHt7Y9asWdDr9Xj44YctfjBfRJSQP378OA4cOAB7e3ts2LABs2fPxrFjx5CZmYnatWujadOmmDBhAq5evQqDwQCDwYBhw4ahY8eOFq35RocOHcJTTz2FHTt24Pfff4evry8MBgNOnDiBhx56CN27d8eECROQlZWF3NxcPPLIIwgNDcVjjz1m1bqNRiPs7OwQFRWF1q1bo2fPngCAsWPHokGDBggKCkJ0dDQ2bNiATZs2WawuVfbsNRoNDAYDZs2ahdLSUvzyyy9wcnKCTqfDgQMHlLDX6XRITk5G27Zt0axZM3Tt2hXdunXD1q1bsXDhQvTp0wceHh5Wfjf/c+PR/D/++AMjR46El5cX+vTpg/nz5yMrKwsdO3aEo6Mj/vzzTwDAk08+idq1a+Oxxx7DCy+8gOrVqwP4X2BUZc/++r2N0tJS5dwFV1dXXLlyBTqdDh4eHtBqtahRowZWr16NF198ETqdDl5eXujQoQP69u0LNzc3ZaNhjj2Pss+yXr166Ny5M65cuYKcnBykpaXh/Pnz+OKLL5SN1TfffIOioiK8/PLL+Pnnn2EymVCtWjV06tQJTk5OsLOzM/tMixuVfb5Lly7FihUr4Onpic6dO2PmzJn4+eefkZWVhXXr1uH3339Ho0aNEBgYiLZt26Jx48YYOXIkGjdubLFaAdz0/3j06FFlA9u8eXMkJSUhKSkJAJCSkoKtW7eiXr16CAwMRLt27dCuXTsMGDAAdevWtWjd10tJScGCBQvQuHFjODs74+OPP0aXLl3QpEkTANe+U1988QXeeustdOjQARs3bkTnzp3LjQyY6/tc9uSqYzAYZPz48RISEiInTpyQ+fPni6enp6xZs0Y++ugjWbdunYiILF68WEJDQ8utO2/ePAkPD5fs7GxrlH5XTp48KQcPHpTTp09LcHCwvPnmm/L+++/Lhg0bxGg0yqeffiqDBg2SwsLCcuuZTCaz1zZ//nwZOXKkrF69WnJzc0VEZNWqVfLPf/5TLl++LCIiGRkZ8t5778mkSZNuqs1gMJi9xjJ//PGHtGrVSoxGo4iI9OzZU2JjY5X2I0eOyEsvvST9+/eXvn37yrFjxyxWW5my2soYDAbZvHmzNG/eXFauXKksP3v2rGRlZSl/jxw5Uvbt22exOu+kuLhYDhw4ICIily9flvj4eAkODpbs7GwxGAxSVFQkIiLZ2dny73//W44ePWrNchUGg0HCw8PljTfekOXLl0txcbGIiMTFxYmfn5/yuOPHj0tISIjk5+dbpU5Vhn12drb07NlTCRkRkdGjR8uQIUNk48aN0rt3bxkwYIAEBgbKoUOHROR/P6ay/0hbMXPmTImLixMRkXPnzkl0dLRs2bJF+UKdPHlS3nrrLVm/fr0YjUYZNGiQfPDBB5KTkyMnTpywyA/9+iA6evSovPbaa/Lxxx/L/v37pVevXjJv3jwpKCiQvLw8GTlypEyfPl1EREpLS+XIkSNy4cIFs9coUn4jZzKZZMaMGZKRkSEiIgMHDpTJkyeLiMiuXbukefPmUlRUJIWFhXLu3DnJzs6WnTt3lns+S22Qrq/74MGDkp6eLsXFxWI0GmXYsGHy0UcfiUj573BSUpIMGTJECVNLu3HjVFxcLPHx8TJw4EC5ePGiiIgcPnxYIiIiZPbs2SIiotfr5euvv5bXX39dpkyZIiUlJRav+0Zbt26VXbt2ydixYyU/P18MBoNcuHBBzp49KyaTSV555RUZO3asfPLJJ+Lr6yvfffddufUt2WlR5TBOjRo18Ouvv6KkpEQ5IFJUVITPP/8cAwYMQHBwMJo3b47Bgwfj4YcfLje+bGsnmhQUFGDOnDlo3Lgx5s2bh9LSUhw8eBDbtm3DSy+9hLS0NGzfvh1hYWHQaDTYsGEDHBwc0LhxY3h6euLRRx81W20Gg0EZsjl27Bh+++03PP7443jmmWfQvXt3fP311zh79ixEBLVq1ULz5s3h6OiIhQsXonPnznBxcYGrqytq165t1t3bsiGW659fo9Hgyy+/xPbt29GrVy80btwYM2fORI8ePdC8eXMcOXIEK1asQFxcHGrWrIkOHTqgQYMGAP43VGWpKZcajQaZmZkYO3YsEhISkJqaih9//FE5uJ2YmIgnn3xSOWj53z4m/4MAAAxASURBVP/+F3PmzMGAAQMQFBRk8QkG1/+eUlNTYTQaUbduXTg7O+PIkSNIT0+Ht7c3nJyccOLECfzxxx/w8vJCbm4ufv31V4SEhKBv375W/S2mpKRg7ty5yMjIgKenJ5YsWYKtW7fiwIEDWLZsGTZt2gRXV1cMHToUNWvWhF6vx7hx49C+fftyz2PRabkW26zYEJPJJMuXL5d33nlHTpw4ISIiX3/9tQwcOFDi4+PLPdaSW97KMBqNNw2zhIWFSY8ePZS9kFOnTkmHDh1kz549cvr0aRk+fLgEBwfLa6+9Jp988okUFBSYpTaTySRFRUXy888/K8uKiopky5YtMmDAAElMTBSj0SgXLlyQt99+W7799lsREXnnnXckNDRUzp8/L6WlpbJr1y6z1Hereq//LI8fP15ueObMmTPSvn17pZ5x48bJ0KFDRUSksLBQfvnlF0lLS7NIrde71Xdy8eLFMmPGDBERuXTpkrz00kuyYMECEbm29zds2DCL1liRjIwMCQ4Olr59+8qHH36o1LdmzRoJCgqSw4cPi4hIfHy8vPHGGzJnzhxrllvOrl27xMPDQ/l8RUT27dsnO3bskIsXL0pmZqZMnz5dfvrpp5vWvdXv11JUGfYi10Jo8uTJEhAQID169JAJEyaYLQSrgslkKrfrq9frlSGltLQ0adGihWzbtk1p//TTTyUgIEBEro3Tzpo1q1yImusLd+LECVm0aJHy97hx46RDhw6SmJioLEtJSZHhw4dLaWmpiIgEBwdLUFCQxUJepPwwQk5OjkyYMEEOHTokTz/9tBw8eFBpmzt3rrz55psiInLhwgXx8PCQP//886bnssQP+MyZM+X+vv5YS9++fWXVqlXK3zt37pQXXnhBjEajpKWlyWuvvSZ79+41e423UtmN0+effy6FhYUyefJk6devn0ybNk369+8vO3bssHTJNyksLJTMzEzlOztgwAAJDg4WkfJDu9nZ2TJnzhzx8/OTlJSUcs9x49CVpak27MucO3dO6UWI3NzbszVXr16VCRMmiK+vrwwZMkQJ0WnTpkm/fv2Ux+Xn58vzzz8vy5cvL7f+jRuNqnDjZ5adnS0TJ04UkWufb/fu3WX16tVK+44dO+T999+X8PBwGThwoISHhytj45ZkMBhkxowZsnDhQqWn9uWXXyrhXlZr69atlbFWax3MvHDhgjRr1kxErm0s//nPf8qYMWPkhx9+EJFrEwfCwsLKPf69996TnJwcKS4utsq4/N1unNq3by9Go1GKi4tl4cKFEhMTYxMTIZYsWSI9e/aU8PBwCQkJEZFr34M2bdrI2bNnlccdPHhQXn/9dZk8ebJNdhxVOWZ/PScnJ9SrV6/c3G9bOSP2xul6K1euxDfffIMnnngCUVFRMBqN+Oyzz/Dcc8/hxRdfxJIlS1C7dm00a9YM9vb26NixI9q3b6+MC4oZplKWzcHXaDTIzc1FdHQ0OnTogA8//BCtWrVCy5YtkZmZid9//x1du3aFvb09GjZsiKZNm+L06dPw8fHB4MGDUatWLfNOO7uBwWBAZGQkcnJy8K9//QuPPvooZs+ejdDQUCQnJ8POzg4tWrTAzp07lamT7du3V45xWLJWAKhduzbOnTuHDRs2ICMjA506dYK7uzvCwsLQr18/PPLII9i4cSOOHj2KZs2aITo6GjVq1ICfnx/s7OwsPi6fkZGBbt26ISgoCAcPHkRQUBBSUlKQn58PT09P5OTk4PDhw+jevTuAa2ewHzlyBM8//zycnJzQpk0btG/f3uonLCYkJODAgQOYPn06nJ2dMW3aNLRu3RrPP/88zp8/j+TkZLz88ssAgPr166NTp07o1asX7O3trXohwVuy6qaGbul2ve+5c+eKh4dHuSlno0aNUqYmLlmyRLy9vW9az9y7jzf2kFesWCErVqyQzp07K6/v5+cnK1asuO1zWHoXt2xG1qVLl5RlH374oYSFhcnOnTule/fu8uqrr0pgYKDFZgNV5OrVq9KsWTP57LPPlGUjR45UxrsPHDggI0eOlICAAJk1a5a1ylSMGTNGRo8eLbGxsfLDDz/IunXrxNPTUy5evCiHDx+WQYMGybRp0yQ7O1tGjRpVbs/EFphMJgkPD5ekpCSJioqSfv36yYYNG5T29PR0adas2U3Dj+bYe64KDHsbc/1wyNGjR2Xq1KmSmJgoRUVFYjAYxMfHR7788kvlMdu2bZM333xTSktLxWQy3TROaG6lpaUybtw4CQkJkdOnT8vChQvl6aefVsZhv/jiCxG5tiHq0aOHMoe+jDWHzIYMGSJLlixR/l63bp14eHhIWlqa5Obm3jRWbAs/4KVLl8qgQYOUv69evSpeXl6yZcsWEbm24b1y5Yq1yivnQds4GQwGiY2NlS+//FL27NkjIiLR0dHi5eVVbuJGSkqKcvzDVs5RqAyGvQ0oLi6W7du3i4goc4dnzpwpb775piQnJ0tgYKC89957yvzo1q1by549eyQ/P19CQkKsOlPhVj3koKAgCQ8Plz179oiHh4dyMszx48etVeZNTCaTrFixQt555x05efKkiFybkfWvf/1L3nrrrXKPtaUZWUajUZ577rlyIRMbG2tzs23KPCgbpz179khkZKSEhobK7NmzpUuXLrJjxw755Zdf5O2331bqXbNmjfTq1Uu++uqrcuvb8nG+MjY0oKReO3fuRGBgIC5fvozq1asjNzcXIoLvv/8eNWrUQGZmJpo3bw47Ozv4+PjA29sbo0ePxqxZs1CrVi0EBgZarXYXFxc0bNgQiYmJyrKePXti+fLlaNiwIfr27YvU1FSICB5//HGbuRy0RqOBn58f3N3dMXbsWHTr1g2ZmZl499130bBhQxQXF990vXpboNVqMX/+fIwfP15ZNmrUKCxYsMCKVd3em2++iZSUFOzfvx/AtXNc3nrrLSxduhTAtc+2Zs2a1iwRmZmZCAgIQL169TBt2jQEBwfDz88Pv/zyCzw9PeHn54dPP/0U77zzDtatW4eoqCi8/fbb5Z7DVo7z3ZF1tzXqdeO43ogRI5Qxy9OnT8sLL7wg/fv3lw8++EAZmik74/fw4cPSqVOncmOF1hpiuF0P2d/fX5mTbuvOnTsn58+fl8uXL8uQIUPk008/tXZJFXr11VeVSwvYeq9y9+7d0qtXL2uXcUcRERHKTBuRazOJ2rVrJ+fOnRORazOJrj+nwtZn7d0Ke/ZWcP39My9evIiCggIEBwfjt99+w8GDB/HYY4/hmWeegZubG+bMmYOnn34aZ86cQVhYGI4fP45mzZqhS5cumDp1qvJ81jrqf7se8uDBg1GvXj0UFRXZ/M2sH374YWzbtg0DBw5Ely5dEBQUZO2SKrRy5Url7G9b71W2adMG9vb2SElJAWCb9w0eM2YMkpOTcebMGQBAo0aN0KJFC+VGKI6OjnB3dwfwv5vA2PrnfiNVXuLYFhiNRsyePRubN29GixYtlCvj/fbbb4iPj8eWLVswadIk+Pv7IysrC3/88Qd69+6NoUOHAgBycnKwYcMG+Pv7W/FdlHf+/HloNBrUqVMHo0aNgpeX1wMRnMC1Swg8+eSTNn//0gdV2WV/bdnSpUsxb948DB8+HN9//z1atmyJ//znP8pVYB90DHsrmTVrFqpVq4agoCDMmzcPS5YsQXx8PIYMGYJRo0ahZ8+e2Lp1K/R6PU6dOoWhQ4fioYceAmD5Od6VVVJSgjVr1uC7775Dv3790L9/f2uXRFRpIoIXXngBAQEB6NOnj8Uv82xuDHsrKCgowNixYzFgwAAsXboU+fn5GDFiBLy8vLB69WpER0fjjz/+uGm9668Fb6vYQ6YH2b59+xAeHo61a9fCZDJBRGx+j6SyOGZvBbVr10ZRURFGjRqFvn37YvHixfDy8sKWLVvg7e2N9u3b46+//iq3jslkgp2dnU0HPQC0aNGCQU8PrGeeeQbVq1fHgQMHoNVq/zZBDwCWvVcaAbi2u+jr64urV6/i8ccfh8lkwmeffYYffvgB4eHhyh16rmdTp10T/Y2tXLnybxXyZTiMYyUlJSWIi4vD4cOHkZ+fD3d3d4wYMQI6nQ6AdWfYENHfD8PeygoLC5GRkVHuPpUMeSKqagx7G8KgJyJzYdgTEakAu5FERCrAsCciUgGGPRGRCjDsiYhUgGFPRKQCDHsiIhVg2BMRqcD/AYXRNDZCU+ozAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gr4QMYEhuAzv"
      },
      "source": [
        "plt.figure(figsize = (20,15))\n",
        "for i , label in enumerate(classes):\n",
        "    for j in range(4) :\n",
        "      plot= plt.subplot(7, 4, 4*i + j+1 )\n",
        "      plt.axis('off')\n",
        "\n",
        "      img_path = os.path.join('/content/input/test/0' , '00' + str(4*i + j+41)+ '.jpg')\n",
        "      img = plt.imread(img_path)\n",
        "      plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5fX_n5p_vz7"
      },
      "source": [
        "plt.figure(figsize = (20,15))\n",
        "for i , label in enumerate(classes):\n",
        "    for j in range(4) :\n",
        "      plot= plt.subplot(7, 4, 4*i + j+1 )\n",
        "      plt.axis('off')\n",
        "\n",
        "      img_path = os.path.join(os.path.join('/content/input/train' , label) , 'pic_00' + str(j+1)+ '.jpg')\n",
        "      img = plt.imread(img_path)\n",
        "      plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXMehx_I3GdU"
      },
      "source": [
        "import os\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import random\n",
        "import wandb \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from importlib import import_module\n",
        "from tqdm import tqdm\n",
        "import timm\n",
        "from PIL import Image\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HizxYYDAMfz"
      },
      "source": [
        "# utils\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkSki6sR3VDg"
      },
      "source": [
        "# seed 고정 \n",
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)  # type: ignore\n",
        "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
        "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "# lr for wandb\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "# save model\n",
        "def save_model(model, version, save_type='acc'):\n",
        "    save_path = os.path.join(f'./ckpts/{version}')\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "\n",
        "    save_dir = os.path.join(save_path, f'best_{save_type}.pth')\n",
        "    torch.save(model.state_dict(), save_dir)\n",
        "\n",
        "################## CUTMIX #################################\n",
        "def rand_bbox(size, lamb):\n",
        "    \"\"\" Generate random bounding box \n",
        "    Args:\n",
        "        - size: [width, breadth] of the bounding box\n",
        "        - lamb: (lambda) cut ratio parameter\n",
        "    Returns:\n",
        "        - Bounding box\n",
        "    \"\"\"\n",
        "    W = size[0]\n",
        "    H = size[1]\n",
        "    cut_rat = np.sqrt(1. - lamb)\n",
        "    cut_w = np.int(W * cut_rat)\n",
        "    cut_h = np.int(H * cut_rat)\n",
        "\n",
        "    # uniform\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "def generate_cutmix_image(image_batch, image_batch_labels, beta):\n",
        "    \"\"\" Generate a CutMix augmented image from a batch \n",
        "    Args:\n",
        "        - image_batch: a batch of input images\n",
        "        - image_batch_labels: labels corresponding to the image batch\n",
        "        - beta: a parameter of Beta distribution.\n",
        "    Returns:\n",
        "        - CutMix image batch, updated labels\n",
        "    \"\"\"\n",
        "    # generate mixed sample\n",
        "    lam = np.random.beta(beta, beta)\n",
        "    rand_index = np.random.permutation(len(image_batch))\n",
        "    target_a = image_batch_labels\n",
        "    target_b = image_batch_labels[rand_index]\n",
        "    bbx1, bby1, bbx2, bby2 = rand_bbox(image_batch[0].shape, lam)\n",
        "    image_batch[:, :, bbx1:bbx2, bby1:bby2] = image_batch[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
        "    \n",
        "    # adjust lambda to exactly match pixel ratio\n",
        "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (image_batch.shape[1] * image_batch.shape[2]))\n",
        "    \n",
        "    return target_a , target_b , lam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMizvDT7ECRT"
      },
      "source": [
        "def make_df():\n",
        "  all_img_files = []\n",
        "  all_labels = []\n",
        "  class_names = {'dog':0 ,'elephant':1 , 'giraffe':2 , 'guitar':3 , 'horse':4 , 'house' : 5 , 'person' : 6}\n",
        "  for name , label in class_names.items():\n",
        "      img_folder = os.path.join('/content/input/train' , name)\n",
        "      for file_name in os.listdir(img_folder):\n",
        "          file_name = os.path.join(img_folder, file_name)\n",
        "          all_img_files.append(file_name)\n",
        "          all_labels.append(label)\n",
        "  \n",
        "  df = pd.DataFrame({'img_path' : all_img_files, 'label' : all_labels})\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqsWHxIDARad"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIUHxKNI5H_N"
      },
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data , transform = None ):\n",
        "      self.data = data  \n",
        "      self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path , label = self.data['img_path'].iloc[idx] , self.data['label'].iloc[idx]\n",
        "        images = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.uint8)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            transformed = self.transform(image=image)\n",
        "            image = transformed[\"image\"]     \n",
        "            return {'image' : image, 'label' : label}\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2F_BJdeAT9D"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1D9OXnNdIL87"
      },
      "source": [
        "# Model \n",
        "class EfficientNet(nn.Module):\n",
        "    def __init__(self , num_classes = 7):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model('efficientnet_b4' , pretrained = True)\n",
        "        self.backbone.classifier = nn.Linear(1792,num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        return x\n",
        "\n",
        "class Swin(nn.Module):\n",
        "    def __init__(self , num_classes = 7):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model('swin_base_patch4_window7_224_in22k' , pretrained = True)\n",
        "        self.backbone.head = nn.Linear(1024,num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        return x\n",
        "\n",
        "class NFNet(nn.Module):\n",
        "    def __init__(self , num_classes = 7):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model('dm_nfnet_f3' , pretrained = True)\n",
        "        self.backbone.head.fc = nn.Linear(3072,num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        return x  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgK3lrjxAfFJ"
      },
      "source": [
        "# Loss\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3SLRZrHLTbe"
      },
      "source": [
        "## LOSS Function\n",
        "# for weighted CE \n",
        "weights = [sum(num_classes)/w for w in num_classes]\n",
        "\n",
        "# LabelSmoothing\n",
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, classes=7, smoothing=0.0, dim=-1):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.cls = classes\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = pred.log_softmax(dim=self.dim)\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
        "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
        "\n",
        "# Focal\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2, alpha=.25, weights=None):   \n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.weight = weights\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        \"\"\"Computes the FocalLoss\n",
        "        Notes: [Batch size,Num classes,Height,Width]\n",
        "        Args:\n",
        "            targets: a tensor of shape [B, H, W] or [B, 1, H, W].\n",
        "            inputs: a tensor of shape [B, C, H, W]. Corresponds to\n",
        "                the raw output or logits of the model. (prediction)\n",
        "        Returns:\n",
        "            focal loss : -alpha *(1-p)*gamma * log(p)\n",
        "        \"\"\"\n",
        "        logp = F.log_softmax(inputs, dim=1)\n",
        "        ce_loss = F.nll_loss(logp, targets, weight=self.weight, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "\n",
        "        loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "        return loss.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g04k5ykcAghj"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmtLt5NYMDsT"
      },
      "source": [
        "def train( train_loader, val_loader,  model,  optimizer ,criterion , scheduler):\n",
        "    \n",
        "    early_stop = EARLY_STOP\n",
        "    cnt = 0\n",
        "    \n",
        "    best_val_acc = 0\n",
        "    best_val_loss = np.inf\n",
        "    \n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        if early_stop < cnt : break\n",
        "\n",
        "        # train loop\n",
        "        model.train()\n",
        "        \n",
        "        train_loss_value = 0\n",
        "        train_acc_value = 0\n",
        "        \n",
        "        with tqdm(train_loader, total = len(train_loader) , unit = 'batch') as train_bar :\n",
        "            for batch, sample in enumerate(train_bar):\n",
        "                images = sample['image'].to(device)\n",
        "                labels = sample['label'].to(device)\n",
        "\n",
        "                r = np.random.rand(1)\n",
        "                if r > CUTMIX and CUTMIX > 0 :\n",
        "                    target_a , target_b , lam = generate_cutmix_image(images , labels , CutMix)\n",
        "                    outputs = model(images)\n",
        "                    preds = outputs.argmax(1)   \n",
        "                    loss = criterion(outs, target_a) * lam + criterion(outs, target_b) * (1. - lam)\n",
        "\n",
        "                else :\n",
        "                    outputs = model(images)\n",
        "                    preds = outputs.argmax(1)            \n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "                train_loss_value += loss.item()\n",
        "                train_acc_value += (preds == labels).sum().item()\n",
        "\n",
        "                train_mean_loss = train_loss_value / TRAIN_BATCHSIZE\n",
        "                train_mean_value = ( train_acc_value / (batch + 1) / TRAIN_BATCHSIZE ) * 100.0\n",
        "\n",
        "                wandb.log({\n",
        "                    'Learning rate': get_lr(optimizer),\n",
        "                    'Train Loss value': train_mean_loss, \n",
        "                    'Train mean acc value': train_mean_value\n",
        "                })\n",
        "\n",
        "                train_bar.set_postfix(trn_loss=train_mean_loss, trn_acc=train_mean_value)\n",
        "\n",
        "        # train loop\n",
        "        model.eval()\n",
        "        \n",
        "        val_loss_value = 0\n",
        "        val_acc_value = 0\n",
        "        with torch.no_grad():\n",
        "            with tqdm(val_loader, total = len(val_loader) , unit = 'batch') as val_bar :\n",
        "                for batch, sample in enumerate(val_bar):\n",
        "                    images = sample['image'].to(device)\n",
        "                    labels = sample['label'].to(device)\n",
        "\n",
        "                    outputs = model(images)\n",
        "                    preds = outputs.argmax(1)            \n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    val_loss_value += loss.item()\n",
        "                    val_acc_value += (preds == labels).sum().item()\n",
        "\n",
        "\n",
        "        val_mean_loss = val_loss_value / VAL_BATCHSIZE\n",
        "        val_mean_acc = (val_acc_value / (batch + 1) / VAL_BATCHSIZE ) * 100.0\n",
        "\n",
        "        val_bar.set_postfix(val_loss=val_mean_loss, val_acc=val_mean_acc)   \n",
        "\n",
        "        wandb.log({\n",
        "            'Val Loss value': val_mean_loss,\n",
        "            'Val mean acc value': val_mean_acc\n",
        "        })\n",
        "\n",
        "        ## save model \n",
        "        if best_val_acc < val_mean_acc:\n",
        "            best_val_acc = val_mean_acc\n",
        "            save_model(model, version = RUN_NAME)\n",
        "        \n",
        "    wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLdr07WPAsCK"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PefpVVALHUhf"
      },
      "source": [
        "# Hyperparamter \n",
        "TRAIN_BATCHSIZE = 16\n",
        "VAL_BATCHSIZE =  16\n",
        "NUM_WORKERS = 3\n",
        "SEED = 42\n",
        "IMG_SIZE =320\n",
        "EARLY_STOP = 5\n",
        "LR = 1e-4\n",
        "NUM_EPOCHS = 20\n",
        "CUTMIX = 0\n",
        "#\n",
        "OPTIMIZER = 'AdamW'\n",
        "SAVE_PATH = '/content/drive/MyDrive/saved'\n",
        "RUN_NAME = \"E4_swin\"\n",
        "# device 설정\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "wandb_config={\"lr\" : LR , \"epochs\": NUM_EPOCHS, \"train_batch_size\": TRAIN_BATCHSIZE , \"val_batch_size\" : VAL_BATCHSIZE , \"num_workers\" : NUM_WORKERS , \"seed\" : SEED , \"img_size\" : IMG_SIZE }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBxc68sr3U5j"
      },
      "source": [
        "# define augmentation\n",
        "train_transform = A.Compose([\n",
        "      A.Resize (IMG_SIZE, IMG_SIZE , p=1),\n",
        "      A.OneOf([\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.Rotate(limit = 30)\n",
        "        ]),\n",
        "      A.RandomResizedCrop (p=1 , height= 224 , width = 224, ratio = (0.75 , 1.25) , scale = (0.7,1.0) ),\n",
        "      A.Cutout(num_holes = 6 , max_h_size = 20 , max_w_size = 20 , p=1),\n",
        "      A.HueSaturationValue (p=0.5),\n",
        "      A.Normalize(),\n",
        "      ToTensorV2()\n",
        "    ])\n",
        "\n",
        "val_transform = A.Compose([\n",
        "        A.Normalize(),\n",
        "        A.Resize (224 , 224 , p=1),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "\n",
        "# dataset 구성\n",
        "dataset = make_df()\n",
        "label = dataset['label']\n",
        "train_x, val_x , train_y , val_y = train_test_split(dataset, label , test_size = 0.2 , random_state = SEED ,shuffle = True, stratify = label)\n",
        "\n",
        "\n",
        "train_dataset = CustomDataset( train_x  , train_transform)\n",
        "val_dataset = CustomDataset( val_x  , val_transform)\n",
        "\n",
        "  \n",
        "# dataloader 구성 \n",
        "train_loader = DataLoader(train_dataset, batch_size = TRAIN_BATCHSIZE, shuffle = True, num_workers = NUM_WORKERS)\n",
        "val_loader = DataLoader(val_dataset, batch_size = VAL_BATCHSIZE, shuffle = True, num_workers = NUM_WORKERS)\n",
        "\n",
        "# model \n",
        "model =  Swin(num_classes=7)\n",
        "model.to(device)\n",
        "\n",
        "# loss\n",
        "criterion = nn.CrossEntropyLoss()  # default: cross_entropy\n",
        "\n",
        "# optimizer \n",
        "if OPTIMIZER == 'MADGRAD' :\n",
        "    optimizer = MADGRAD(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                        lr=LR,\n",
        "                        weight_decay=5e-4)\n",
        "else :\n",
        "    opt_module = getattr(import_module(\"torch.optim\"), OPTIMIZER)  # default: SGD\n",
        "    optimizer = opt_module(\n",
        "        filter(lambda p: p.requires_grad, model.parameters()),\n",
        "        lr=LR,\n",
        "        weight_decay=5e-4\n",
        "    )\n",
        "\n",
        "# scheduler\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = 1000 , T_mult = 2 , eta_min = 5e-5 )\n",
        "\n",
        "# TRAIN\n",
        "wandb.login()\n",
        "wandb.init(project='Image classficiation', name=RUN_NAME, config = wandb_config)\n",
        "train(train_loader, val_loader , model , optimizer ,criterion , scheduler )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrH4s5a7KVUu"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZN6vBJdWrOJ"
      },
      "source": [
        "# INFERENCE DATASETCODE\n",
        "class TestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_path , transform = None ):\n",
        "      self.data_path = data_path\n",
        "      self.data = sorted(os.listdir('/content/input/test/0'))\n",
        "      self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.data_path , self.data[idx])\n",
        "        images = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.uint8)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            transformed = self.transform(image=image)\n",
        "            image = transformed[\"image\"]     \n",
        "            return {'image' : image , 'image_path' : image_path}\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hReXLXCyTw7p",
        "outputId": "aa7d3cdc-a034-4b82-bee4-e82b6c30ff9d"
      },
      "source": [
        "# INFERENCE\n",
        "test_transform = A.Compose([\n",
        "        A.Normalize(),\n",
        "        A.Resize (224 , 224 , p=1),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "test_dataset = TestDataset('/content/input/test/0' , test_transform)\n",
        "\n",
        "# dataloader 구성 \n",
        "test_loader = DataLoader(test_dataset, batch_size = 1, shuffle = False, num_workers = NUM_WORKERS)\n",
        "\n",
        "model1 = Swin(num_classes=  7)\n",
        "model1.load_state_dict(torch.load('/content/ckpts/E4_swin/best_acc.pth'))\n",
        "model1.to(device)\n",
        "\n",
        "model2 = NFNet(num_classes = 7)\n",
        "model2.load_state_dict(torch.load('/content/ckpts/E4_NFNet3/best_acc.pth'))\n",
        "model2.to(device)\n",
        "submission = []\n",
        "model1.eval()\n",
        "model2.eval()\n",
        "with torch.no_grad():\n",
        "    with tqdm(test_loader, total = len(test_loader) , unit = 'batch') as test_bar :\n",
        "        for batch, sample in enumerate(test_bar):\n",
        "            images = sample['image'].to(device)\n",
        "            image_path = sample['image_path']\n",
        "\n",
        "            # outputs1= model1(images)\n",
        "            # outputs2 =model2(images)\n",
        "            # outputs = (outputs1 + outputs2) / 2\n",
        "            outputs = model(images)\n",
        "            preds = outputs.argmax(1)\n",
        "            submission.extend(preds.cpu().numpy())\n",
        "\n",
        "submission.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 350/350 [00:27<00:00, 12.52batch/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YUknQVhYxCo"
      },
      "source": [
        "submission_csv = pd.read_csv('/content/input/test_answer_sample_.csv',index_col = 0)\n",
        "submission_csv['answer value'] = submission\n",
        "submission_csv.head()\n",
        "submission_csv.to_csv('/content/input/swin.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5_IgkOCRoTJ"
      },
      "source": [
        "# visualization for augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqfqjNoo1DH4"
      },
      "source": [
        "image = cv2.imread('/content/input/test/0/0001.jpg')\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "def visualize(image):\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image)\n",
        "\n",
        "visualize(image)\n",
        "\n",
        "transform = A.Compose([\n",
        "                      A.HueSaturationValue (p=1)\n",
        "                       ])\n",
        "augmented_image = transform(image=image)['image']\n",
        "visualize(augmented_image)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}